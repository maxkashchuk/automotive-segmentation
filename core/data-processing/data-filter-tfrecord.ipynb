{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging as lg\n",
    "import datetime as dt\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "\n",
    "GLOBAL_TIMESTAMP = '{:%Y_%m_%d_%H_%M_%S}'.format(dt.datetime.now())\n",
    "\n",
    "LINES_FILE_EXTENSIONS_FORMAT = \".lines.txt\"\n",
    "\n",
    "BASE_PATH = \"../../\"\n",
    "\n",
    "OUTPUT_CSV_PATH = f\"{BASE_PATH}dataset-description\"\n",
    "\n",
    "data_split_path = {\"train\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"validation\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"test\": f\"datasets/CuLane/test\"}\n",
    "\n",
    "data_split_labels_path = {\"train\": f\"dataset-description/train.txt\",\n",
    "                          \"validation\": f\"dataset-description/val.txt\",\n",
    "                          \"test\": f\"dataset-description/test.txt\"}\n",
    "\n",
    "TRAIN_LOSS = VALIDATION_LOSS = TEST_LOSS = 0\n",
    "\n",
    "def data_frame_log(error_line, data_label):\n",
    "    global GLOBAL_TIMESTAMP\n",
    "\n",
    "    logging_path = f\"{OUTPUT_CSV_PATH}/data-filter-logging/{data_label}_{GLOBAL_TIMESTAMP}_.log\"\n",
    "\n",
    "    lg.basicConfig(filename=f\"{logging_path}\", filemode=\"w\", level=lg.INFO)\n",
    "\n",
    "    lg.info(f\"[{GLOBAL_TIMESTAMP}] | \" + error_line)\n",
    "\n",
    "def data_frames_obtain(train_frame, validation_frame, test_frame):\n",
    "    train_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['train']}\", sep=\" \", header=None)\n",
    "\n",
    "    validation_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['validation']}\", sep=\" \", header=None)\n",
    "\n",
    "    test_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['test']}\", sep=\" \", header=None)\n",
    "\n",
    "    return train_frame, validation_frame, test_frame\n",
    "\n",
    "def data_frames_filter(data_base_path, data_frame, data_label):\n",
    "    global TRAIN_LOSS, VALIDATION_LOSS, TEST_LOSS, BASE_PATH\n",
    "\n",
    "    data_frame_copy = data_frame.copy()\n",
    "\n",
    "    Path(f\"{OUTPUT_CSV_PATH}/data-filter-logging\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ind, path_frame in tqdm(enumerate(data_frame_copy[0]), total=len(data_frame_copy[0]), desc=f\"{data_label} data filtering\", colour=\"GREEN\"):\n",
    "        path = Path(f\"{BASE_PATH}{data_base_path}{path_frame}\")\n",
    "\n",
    "        path_lane = path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)\n",
    "\n",
    "        file_id = path.stem\n",
    "\n",
    "        id_record_flag = False\n",
    "\n",
    "        line_file_flag = False\n",
    "\n",
    "        if not path.exists():\n",
    "            data_frame_log(f\"Image file index: [{file_id}] | According to path [{path}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "        \n",
    "        elif not path_lane.exists():\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif path_lane.stat().st_size == 0:\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Is empty\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif False == line_file_flag:\n",
    "            with open(path_lane, 'r') as file:\n",
    "                lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "            \n",
    "            if 2 > len(lines):\n",
    "                data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Line quantity below minimum [2]\", data_label)\n",
    "\n",
    "                id_record_flag = True\n",
    "        \n",
    "        if id_record_flag:\n",
    "            data_frame.drop(ind, inplace=True)\n",
    "\n",
    "            if list(data_split_path.keys())[0] == data_label:\n",
    "                TRAIN_LOSS += 1\n",
    "            elif list(data_split_path.keys())[1] == data_label:\n",
    "                VALIDATION_LOSS += 1\n",
    "            else:\n",
    "                TEST_LOSS += 1\n",
    "\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def data_process_mask(lines_file, image_file):\n",
    "    mask_wdith, mask_height, _ = cv.imread(image_file).shape\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    mask = np.zeros((mask_wdith, mask_height), dtype=np.uint8)\n",
    "\n",
    "    line_id = lane_id = coords_first_id = coords_second_id = 0\n",
    "\n",
    "    with open(lines_file, 'r') as file:\n",
    "        lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "    \n",
    "    while line_id != len(lines) - 1:\n",
    "        coords_first_id = coords_second_id = 0\n",
    "\n",
    "        while coords_first_id + 4 <= len(lines[line_id]) or coords_second_id + 4 <= len(lines[line_id + 1]):\n",
    "            x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "            x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "            x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "            x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "            cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "            if coords_first_id + 4 < len(lines[line_id]):\n",
    "                    coords_first_id += 2\n",
    "            \n",
    "            if coords_second_id + 4 < len(lines[line_id + 1]):\n",
    "                    coords_second_id += 2\n",
    "\n",
    "            if coords_first_id + 4 == len(lines[line_id]) and coords_second_id + 4 == len(lines[line_id + 1]):\n",
    "                    x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "                    x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "                    x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "                    x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "                    cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "                    break\n",
    "\n",
    "        line_id += 1\n",
    "\n",
    "        lane_id += 1\n",
    "\n",
    "    return np.stack([mask] * 3, axis=-1), lane_id\n",
    "\n",
    "def resize_and_pad_mask(mask, target_size=512, pad_value=255):\n",
    "    mask = mask[:, :, 0]\n",
    "\n",
    "    original_height, original_width = mask.shape\n",
    "\n",
    "    scale = min(target_size / original_height, target_size / original_width)\n",
    "    new_height = int(round(original_height * scale))\n",
    "    new_width = int(round(original_width * scale))\n",
    "\n",
    "    resized = cv.resize(mask, (new_width, new_height), interpolation=cv.INTER_NEAREST)\n",
    "\n",
    "    pad_top = (target_size - new_height) // 2\n",
    "    pad_bottom = target_size - new_height - pad_top\n",
    "    pad_left = (target_size - new_width) // 2\n",
    "    pad_right = target_size - new_width - pad_left\n",
    "\n",
    "    padded = cv.copyMakeBorder(\n",
    "        resized,\n",
    "        pad_top, pad_bottom,\n",
    "        pad_left, pad_right,\n",
    "        borderType=cv.BORDER_CONSTANT,\n",
    "        value=pad_value\n",
    "    )\n",
    "\n",
    "    return padded.astype(np.uint8)\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def serialize_example(image_path, mask_tensor, lane_quantity):\n",
    "    feature = {\n",
    "        'image_path': _bytes_feature(image_path.encode('utf-8')),\n",
    "        'mask_raw': _bytes_feature(mask_tensor.tobytes()),\n",
    "        'height': _int64_feature(mask_tensor.shape[0]),\n",
    "        'width': _int64_feature(mask_tensor.shape[1]),\n",
    "        'lane_quantity': _int64_feature(lane_quantity)\n",
    "    }\n",
    "\n",
    "    record_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "    return record_proto.SerializeToString()\n",
    "\n",
    "def tensors_formation(data_base_path, data_frame, data_label, shard_size=1000):\n",
    "    tfrecord_path = f\"{OUTPUT_CSV_PATH}/{data_label}.tfrecord\"\n",
    "    Path(tfrecord_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tfrecord_data_label_path = f\"{OUTPUT_CSV_PATH}/{data_label}\"\n",
    "    Path(tfrecord_data_label_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tfrecord_dir = Path(f\"{OUTPUT_CSV_PATH}/{data_label}\")\n",
    "    tfrecord_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    options = tf.io.TFRecordOptions(compression_type=\"ZLIB\")\n",
    "    total_samples = len(data_frame)\n",
    "    num_shards = (total_samples + shard_size - 1) // shard_size\n",
    "\n",
    "    for shard_id in range(num_shards):\n",
    "        start_idx = shard_id * shard_size\n",
    "        end_idx = min((shard_id + 1) * shard_size, total_samples)\n",
    "\n",
    "        shard_filename = tfrecord_dir / f\"{data_label}_{shard_id:05d}-of-{num_shards:05d}.tfrecord\"\n",
    "        with tf.io.TFRecordWriter(str(shard_filename), options=options) as writer:\n",
    "            for ind in tqdm(range(start_idx, end_idx), desc=f\"{data_label} shard {shard_id+1}/{num_shards}\", colour=\"BLUE\"):\n",
    "                try:\n",
    "                    general_image_file_path = Path(data_base_path + data_frame[0][ind])\n",
    "                    image_file_path = Path(f\"{BASE_PATH}{general_image_file_path}\")\n",
    "                    line_file_path = Path(f\"{BASE_PATH}{general_image_file_path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)}\")\n",
    "\n",
    "                    mask, lanes = data_process_mask(line_file_path, image_file_path)\n",
    "                    mask = resize_and_pad_mask(mask)\n",
    "                    serialized_example = serialize_example(str(general_image_file_path), mask, lanes)\n",
    "                    writer.write(serialized_example)\n",
    "                except Exception as e:\n",
    "                    data_frame_log(f\"Serialization error at index {ind}: {str(e)}\", data_label)\n",
    "\n",
    "train_frame = validation_frame = test_frame = None\n",
    "\n",
    "train_frame, validation_frame, test_frame = data_frames_obtain(train_frame, validation_frame, test_frame)\n",
    "\n",
    "data_frames_filter(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "data_frames_filter(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "data_frames_filter(data_split_path['test'], test_frame, list(data_split_path.keys())[2])\n",
    "\n",
    "statistics_df = pd.DataFrame({\"train_loss\": TRAIN_LOSS, \"validation_loss\": VALIDATION_LOSS, \"test_loss\": TEST_LOSS})\n",
    "\n",
    "statistics_df.to_csv(f\"{OUTPUT_CSV_PATH}/loss_statistics.csv\")\n",
    "\n",
    "tensors_formation(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "tensors_formation(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "tensors_formation(data_split_path['test'], test_frame, list(data_split_path.keys())[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-os",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
