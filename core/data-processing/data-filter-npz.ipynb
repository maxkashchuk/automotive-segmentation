{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging as lg\n",
    "import datetime as dt\n",
    "import cv2 as cv\n",
    "\n",
    "GLOBAL_TIMESTAMP = '{:%Y_%m_%d_%H_%M_%S}'.format(dt.datetime.now())\n",
    "\n",
    "LINES_FILE_EXTENSIONS_FORMAT = \".lines.txt\"\n",
    "\n",
    "BASE_PATH = \"../../\"\n",
    "\n",
    "OUTPUT_CSV_PATH = f\"{BASE_PATH}dataset-description\"\n",
    "\n",
    "data_split_path = {\"train\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"validation\": f\"datasets/CuLane/train-validation\",\n",
    "                   \"test\": f\"datasets/CuLane/test\"}\n",
    "\n",
    "data_split_labels_path = {\"train\": f\"dataset-description/train.txt\",\n",
    "                          \"validation\": f\"dataset-description/val.txt\",\n",
    "                          \"test\": f\"dataset-description/test.txt\"}\n",
    "\n",
    "TRAIN_LOSS = VALIDATION_LOSS = TEST_LOSS = 0\n",
    "\n",
    "def data_frame_log(error_line, data_label):\n",
    "    global GLOBAL_TIMESTAMP\n",
    "\n",
    "    logging_path = f\"{OUTPUT_CSV_PATH}/data-filter-logging/{data_label}_{GLOBAL_TIMESTAMP}_.log\"\n",
    "\n",
    "    lg.basicConfig(filename=f\"{logging_path}\", filemode=\"w\", level=lg.INFO)\n",
    "\n",
    "    lg.info(f\"[{GLOBAL_TIMESTAMP}] | \" + error_line)\n",
    "\n",
    "def data_frames_obtain(train_frame, validation_frame, test_frame):\n",
    "    train_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['train']}\", sep=\" \", header=None)\n",
    "\n",
    "    validation_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['validation']}\", sep=\" \", header=None)\n",
    "\n",
    "    test_frame = pd.read_csv(f\"{BASE_PATH}{data_split_labels_path['test']}\", sep=\" \", header=None)\n",
    "\n",
    "    return train_frame, validation_frame, test_frame\n",
    "\n",
    "def data_frames_filter(data_base_path, data_frame, data_label):\n",
    "    global TRAIN_LOSS, VALIDATION_LOSS, TEST_LOSS, BASE_PATH\n",
    "\n",
    "    data_frame_copy = data_frame.copy()\n",
    "\n",
    "    Path(f\"{OUTPUT_CSV_PATH}/data-filter-logging\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ind, path_frame in tqdm(enumerate(data_frame_copy[0]), total=len(data_frame_copy[0]), desc=f\"{data_label} data filtering\", colour=\"GREEN\"):\n",
    "        path = Path(f\"{BASE_PATH}{data_base_path}{path_frame}\")\n",
    "\n",
    "        path_lane = path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)\n",
    "\n",
    "        file_id = path.stem\n",
    "\n",
    "        id_record_flag = False\n",
    "\n",
    "        line_file_flag = False\n",
    "\n",
    "        if not path.exists():\n",
    "            data_frame_log(f\"Image file index: [{file_id}] | According to path [{path}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "        \n",
    "        elif not path_lane.exists():\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Does not exist\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif path_lane.stat().st_size == 0:\n",
    "            data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Is empty\", data_label)\n",
    "\n",
    "            id_record_flag = True\n",
    "\n",
    "            line_file_flag = True\n",
    "        \n",
    "        elif False == line_file_flag:\n",
    "            with open(path_lane, 'r') as file:\n",
    "                lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "            \n",
    "            if 2 > len(lines):\n",
    "                data_frame_log(f\"Line file index: [{file_id}] | According to path [{path_lane}] | Line quantity below minimum [2]\", data_label)\n",
    "\n",
    "                id_record_flag = True\n",
    "        \n",
    "        if id_record_flag:\n",
    "            data_frame.drop(ind, inplace=True)\n",
    "\n",
    "            if list(data_split_path.keys())[0] == data_label:\n",
    "                TRAIN_LOSS += 1\n",
    "            elif list(data_split_path.keys())[1] == data_label:\n",
    "                VALIDATION_LOSS += 1\n",
    "            else:\n",
    "                TEST_LOSS += 1\n",
    "\n",
    "    data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def data_process_mask(lines_file, image_file):\n",
    "    mask_wdith, mask_height, _ = cv.imread(image_file).shape\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    mask = np.zeros((mask_wdith, mask_height), dtype=np.uint8)\n",
    "\n",
    "    line_id = lane_id = coords_first_id = coords_second_id = 0\n",
    "\n",
    "    with open(lines_file, 'r') as file:\n",
    "        lines = [list(map(int, map(float, line.split()))) for line in file]\n",
    "    \n",
    "    while line_id != len(lines) - 1:\n",
    "        coords_first_id = coords_second_id = 0\n",
    "\n",
    "        while coords_first_id + 4 <= len(lines[line_id]) or coords_second_id + 4 <= len(lines[line_id + 1]):\n",
    "            x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "            x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "            x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "            x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "            cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "            if coords_first_id + 4 < len(lines[line_id]):\n",
    "                    coords_first_id += 2\n",
    "            \n",
    "            if coords_second_id + 4 < len(lines[line_id + 1]):\n",
    "                    coords_second_id += 2\n",
    "\n",
    "            if coords_first_id + 4 == len(lines[line_id]) and coords_second_id + 4 == len(lines[line_id + 1]):\n",
    "                    x1, y1 = lines[line_id][coords_first_id], lines[line_id][coords_first_id + 1]\n",
    "                    x2, y2 = lines[line_id][coords_first_id + 2], lines[line_id][coords_first_id + 3]\n",
    "                    x3, y3 = lines[line_id + 1][coords_second_id], lines[line_id + 1][coords_second_id + 1]\n",
    "                    x4, y4 = lines[line_id + 1][coords_second_id + 2], lines[line_id + 1][coords_second_id + 3]\n",
    "\n",
    "                    cv.fillPoly(mask, [np.array([[x1, y1], [x2, y2], [x4, y4], [x3, y3]])], lane_id + 1, lineType = cv.LINE_AA)\n",
    "\n",
    "                    break\n",
    "\n",
    "        line_id += 1\n",
    "\n",
    "        lane_id += 1\n",
    "\n",
    "    return np.stack([mask] * 3, axis=-1), lane_id\n",
    "\n",
    "def tensors_formation(data_base_path, data_frame, data_label):\n",
    "    df = pd.DataFrame(columns=[\"image_path\", \"tensor_path\", \"lane_quantity\"])\n",
    "\n",
    "    tensors_storage = f\"{OUTPUT_CSV_PATH}/{data_label}\"\n",
    "\n",
    "    Path(f\"{tensors_storage}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ind, elem in tqdm(enumerate(data_frame[0]), total=len(data_frame[0]), desc=f\"{data_label} tensor formation\", colour=\"BLUE\"):        \n",
    "        general_image_file_path = Path(data_base_path + elem)\n",
    "\n",
    "        image_file_path = Path(f\"{BASE_PATH}{general_image_file_path}\")\n",
    "\n",
    "        line_file_path = Path(f\"{BASE_PATH}{general_image_file_path.with_suffix(LINES_FILE_EXTENSIONS_FORMAT)}\")\n",
    "\n",
    "        mask, lanes = data_process_mask(line_file_path, image_file_path)\n",
    "\n",
    "        mask_tensor = np.array(mask)\n",
    "\n",
    "        row = pd.DataFrame({\n",
    "            \"image_path\": general_image_file_path,\n",
    "            \"tensor_path\": f\"{elem}.npz\",\n",
    "            \"lane_quantity\": lanes\n",
    "        }, index = [ind])\n",
    "\n",
    "        Path(f\"{tensors_storage}{elem}\").parent.mkdir(parents = True, exist_ok = True)\n",
    "\n",
    "        np.savez_compressed(f\"{tensors_storage}{elem}\", mask_tensor)\n",
    "\n",
    "        df = pd.concat([df, row])\n",
    "    \n",
    "    df.to_csv(f\"{OUTPUT_CSV_PATH}/{data_label}.csv\", index=True)\n",
    "\n",
    "train_frame = validation_frame = test_frame = None\n",
    "\n",
    "train_frame, validation_frame, test_frame = data_frames_obtain(train_frame, validation_frame, test_frame)\n",
    "\n",
    "data_frames_filter(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "data_frames_filter(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "data_frames_filter(data_split_path['test'], test_frame, list(data_split_path.keys())[2])\n",
    "\n",
    "# statistics_df = pd.DataFrame({\"train_loss\": TRAIN_LOSS, \"validation_loss\": VALIDATION_LOSS, \"test_loss\": TEST_LOSS})\n",
    "\n",
    "# statistics_df.to_csv(f\"{OUTPUT_CSV_PATH}/loss_statistics.csv\")\n",
    "\n",
    "tensors_formation(data_split_path['train'], train_frame, list(data_split_path.keys())[0])\n",
    "\n",
    "tensors_formation(data_split_path['validation'], validation_frame, list(data_split_path.keys())[1])\n",
    "\n",
    "tensors_formation(data_split_path['test'], test_frame, list(data_split_path.keys())[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto-os",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
